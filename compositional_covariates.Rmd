---
title: "Compositional covariates"
author: "Drew Tyre"
date: "5/9/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The problem with compositional covariates is that they create multi-collinearity. We'll simulate the dataset created by Brian Cade to illustrate the problem and explore the different possible fixes.

```{r}
library(tidyverse)
library(GGally)
# doesn't matter what this is -- if you use a different number your results will be different from mine.
set.seed(87)
df <- data_frame(
  pos.tot = runif(200,min=0.8,max=1.0),
  urban.tot = pmin(runif(200,min=0.0,max=0.02),1.0 - pos.tot),
  neg.tot = (1.0 - pmin(pos.tot + urban.tot,1)),
  x1= pmax(pos.tot - runif(200,min=0.05,max=0.30),0),
  x3= pmax(neg.tot - runif(200,min=0.0,max=0.10),0),
  x2= pmax(pos.tot - x1 - x3/2,0),
  x4= pmax(1 - x1 - x2 - x3 - urban.tot,0))

ggpairs(df)

```

So there is a near perfect negative correlation between the things sagegrouse like and the things they don't like, although it gets less bad when considering the individual covariates. In fact, looking at the correlations between just x1 through x4 none of them have correlations bigger than $|0.7|$, so common "rules of thumb" would not exclude them. Now we'll build up a poisson response, and fit all the models 
```{r}
mean.y <- exp(-5.8 + 6.3*df$x1 + 15.2*df$x2)
df$y <- rpois(200,mean.y)

library(MuMIn)
globalMod <- glm(y ~ x1 + x2 + x3 + x4, data = df, na.action = na.fail, family=poisson())
fits <- dredge(globalMod)
model.sel(fits)
```

That's some crazy stuff! Depending on which other variables are in the model, the coefficients for x1 and x2 can even be negative, although those models have a very low weight. The coefficients for x3 and x4 (things grouse don't like) are in fact positive in all the top models. 

Does looking at model averaged coefficients help?

```{r}
summary(model.avg(fits))
```

This is better -- at least the estimates for x1 and x2 are in the right ballpark, and they are considered significantly different from zero. x3 and x4, although positive, are not significantly different from zero. 

## how do we fix the problem? 

There are a few possible fixes for compositional covariates

- leave one out. This only really helps when there are two categories. In this case we have left out urban cover, and as a result we have kind of already done it, and it's not helping.
- Reduce correlations with PCA and then use transformed axes
- use the isometric log ratio transformation

### Using PCA

```{r}
df2 <- df[, 4:7]
grouse_pca <- princomp(df2)
plot(grouse_pca)
```

and the summary

```{r}
summary(grouse_pca)
```

shows us that 95% of the variation is included in the first two components. We will use all 4 components and refit the models used above.

```{r}
df_pca <- as_data_frame(grouse_pca$scores)
df_pca$y <- df$y
globalMod_pca <- glm(y ~ Comp.1 + Comp.2 + Comp.3 + Comp.4, data = df_pca, na.action = na.fail, family=poisson())
fits <- dredge(globalMod_pca)
model.sel(fits)
```

So it looks like the first two components are pretty much carrying all the water together, but not separately. We can confirm this by looking at the model averaged coefficients.

```{r}
summary(model.avg(fits))
```


Now comes the hard part. What does that actually mean in terms of the original variables?

```{r}
biplot(grouse_pca)
```

So component 1 is "heavily loaded" on x1, and partially negative on x2. The arrow for x2 is not parallel to the x axis. The second component is mostly reflecting x3 and x4, but is partially negative of x2. Another way to see this is to look at the loadings directly

```{r}
grouse_pca$loadings
```

So the real hurdle is figuring out what those coefficients actually mean. Probably the best way would be to create a new data frame reflecting some set of possible changes in habitat proportions (appropriately constrained), using the loadings matrix to transform those into the PCA scores, and then using that for prediction. 

```{r}
nd <- data_frame(x1 = seq(0.52, 0.85, 0.01),
                 x2 = 0.15,
                 pos = x1 + x2,
                 x3 = pmax(1-pos,0),
                 x4 = pmax(1-pos-x3,0))
nd_pca <- as_data_frame(predict(grouse_pca, nd[,-3]))
predmodel <- get.models(fits, subset = 2)
preds <- cbind(nd,
               meancount = predict(predmodel[[1]], newdata=nd_pca, type = "response"))
ggplot(preds, aes(x = x1, y=meancount)) + geom_line() + 
  geom_point(aes(x = x1, y = y, color = x2), data=df)
```

So, not trivial to figure out how to make predictions using these models. But seems to solve the issues.

### Isometric log ratio transformations

```{r}
library(robCompositions)
pivotCoord(as.data.frame(df2))
```

So the problem is that this transformation can't handle zeros. Hmmm.